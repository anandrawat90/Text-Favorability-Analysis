{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "#import tensorboard\n",
    "from keras.callbacks import TensorBoard\n",
    "#for plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants for running the network\n",
    "MAX_NUM_WORDS = 20000 #only consider top 200 \n",
    "EMBEDDING_DIM = 200\n",
    "TWEET_LENGTH = 140 #max length of sequence\n",
    "VALIDATION_SPLIT = 0.20 #validation to split, 80% testing 20% validation\n",
    "MODEL_TYPE_ID = ''\n",
    "MASTER_DIR = '../../'\n",
    "MODELS_DIR = path.join(MASTER_DIR,'Models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading dataset for the analysis\n",
    "dataset = pd.read_csv('../../Sentiment Analysis Data/input/Processed_Sentiment Analysis Dataset 2.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading tweets and creating target labels\n",
    "tweets = dataset.SentimentText.tolist() # tweets\n",
    "labels = dataset.Sentiment.tolist() # target labels\n",
    "labels_dict = {1:\"Positive\",0:\"Negative\"} #mapping to labels to meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 1578614 number of tweets\n",
      "found 1578614 number of labels\n",
      "first 5 tweets from the dataset\n",
      " is so sad for my APL friend.............\n",
      "I missed the New Moon trailer...\n",
      "omg its already 7:30 :O\n",
      ".. Omgaga. Im sooo im gunna CRy. I've been at this dentist since 11.. I was suposed 2 just get a crown put on (30mins)...\n",
      "i think mi bf is cheating on me!!! T_T\n"
     ]
    }
   ],
   "source": [
    "print('found %s number of tweets' %len(tweets))\n",
    "print('found %s number of labels' %len(labels))\n",
    "print('first 5 tweets from the dataset\\n','\\n'.join(tweets[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import utilities for pre-processing the tweets to make them ready for RNN\n",
    "from keras.preprocessing.text import Tokenizer # required for tokenizing the tweets i.e., breaking it into word array\n",
    "from keras.preprocessing.sequence import pad_sequences # required for padding the tweets to be of a particular length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,tweet in enumerate(tweets):\n",
    "    if not isinstance(tweet,str):\n",
    "        print(index,tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only consider top 200 words in the dataset(by frequency)\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert each tweet to a sequences and therefore converting the dataset of tweets to a list of sequences \n",
    "sequences = tokenizer.texts_to_sequences(tweets)\n",
    "#creating padded sequences of length_tweet\n",
    "data = pad_sequences(sequences, maxlen=TWEET_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of unique words found 635095\n"
     ]
    }
   ],
   "source": [
    "#retrieve the word index i.e., {'<word>':'<index>'} of the current dataset\n",
    "word_index = tokenizer.word_index\n",
    "print('total number of unique words found %s' %len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of data tensor: (1578614, 140)\n",
      "the shape of label tensor: (1578614, 2)\n"
     ]
    }
   ],
   "source": [
    "#Converts a class vector (integers) labels to binary class matrix.\n",
    "from keras.utils import to_categorical\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "\n",
    "print('the shape of data tensor:',data.shape)\n",
    "print('the shape of label tensor:',labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data into training and testing dataset\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices) #randomly move the indices\n",
    "data = data[indices] #sync with random shuffle in the previous step\n",
    "labels = labels[indices] #sync with the shuffle\n",
    "\n",
    "#determine the spliting point\n",
    "split_point = int(VALIDATION_SPLIT * data.shape[0])\n",
    "x_train = data[:-split_point]\n",
    "y_train = labels[:-split_point]\n",
    "x_test = data[-split_point:]\n",
    "y_test = labels[-split_point:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Preparing the embedded layer </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total vocab found 1193514\n"
     ]
    }
   ],
   "source": [
    "#create dictionary of words and their vectors using the GloVe embeddings\n",
    "embedding_index = {}\n",
    "with open(path.join(MASTER_DIR,'Word Embedding/glove.twitter.27B.200d.txt'),encoding='utf-8') as glove_embedding:\n",
    "    for line in glove_embedding:\n",
    "        emb_data = line.split()\n",
    "        word = emb_data[0]\n",
    "        coefs = np.asarray(emb_data[1:],dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "        \n",
    "print('total vocab found',len(embedding_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: i rank: 1\n",
      "word: to rank: 2\n",
      "word: the rank: 3\n",
      "word: a rank: 4\n",
      "word: my rank: 5\n"
     ]
    }
   ],
   "source": [
    "#show top five words from word_index\n",
    "i = 0 \n",
    "for key,value in word_index.items():\n",
    "    if i < 5:\n",
    "        print('word:',key,'rank:',value)\n",
    "        i+=1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words = min(MAX_NUM_WORDS,len(word_index))\n",
    "#use embeddding_index and word_index to compute embedding_matrix\n",
    "embedding_matrix = np.zeros((nb_words,EMBEDDING_DIM))\n",
    "word_not_in_embedding = []\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        #words not found in embedding_index will be all zeros\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        #create a list of words not in embedding to statistical analysis\n",
    "        word_not_in_embedding.append((word,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "632515 number of words found and 2580 number of words missed\n"
     ]
    }
   ],
   "source": [
    "nb_missed_words = len(word_not_in_embedding)\n",
    "nb_found_words = len(word_index) - nb_missed_words\n",
    "print('%s number of words found and %s number of words missed'%(nb_found_words,nb_missed_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Create Embedding Layer for the Network</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "#set trainable to false as the embedding layer should not be trained during back propogation i.e., training\n",
    "embedding_layer = Embedding(nb_words,EMBEDDING_DIM,trainable=False,\n",
    "                            weights=[embedding_matrix],input_length = TWEET_LENGTH,name='embedding_layer_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training a 1D Convolution Net a.k.a 1D ConvNet</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer_1 (Embedding (None, 140, 200)          4000000   \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 140, 64)           89664     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 70, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 70, 64)            28736     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_6 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 4,120,546\n",
      "Trainable params: 120,546\n",
      "Non-trainable params: 4,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import Input, Sequential, regularizers, optimizers\n",
    "from keras.layers import Conv1D, MaxPooling1D,Dense,GlobalMaxPooling1D,Dropout\n",
    "\n",
    "#training params\n",
    "default_batch_size = 256 \n",
    "num_epochs = 8 \n",
    "\n",
    "#model parameters\n",
    "num_filters = 64\n",
    "embed_dim = 300 \n",
    "weight_decay = 1e-4\n",
    "\n",
    "MODEL_TYPE_ID = '1D_Conv_Net'\n",
    "embedding_layer_map = {'embedding_layer_1':path.join(MASTER_DIR,'Word Embedding/glove.twitter.27B.200d.txt')}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Dense(len(labels_dict), activation='softmax'))  #\n",
    "\n",
    "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Start Training </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1262892 samples, validate on 315722 samples\n",
      "Epoch 1/8\n",
      "1262892/1262892 [==============================] - 102s 81us/step - loss: 0.4511 - acc: 0.7892 - val_loss: 0.4250 - val_acc: 0.8059\n",
      "Epoch 2/8\n",
      "1262892/1262892 [==============================] - 95s 75us/step - loss: 0.4164 - acc: 0.8095 - val_loss: 0.4146 - val_acc: 0.8104\n",
      "Epoch 3/8\n",
      "1262892/1262892 [==============================] - 97s 77us/step - loss: 0.4037 - acc: 0.8165 - val_loss: 0.4095 - val_acc: 0.8143\n",
      "Epoch 4/8\n",
      "1262892/1262892 [==============================] - 97s 77us/step - loss: 0.3951 - acc: 0.8213 - val_loss: 0.4124 - val_acc: 0.8138\n",
      "Epoch 5/8\n",
      "1262892/1262892 [==============================] - 97s 77us/step - loss: 0.3882 - acc: 0.8252 - val_loss: 0.4086 - val_acc: 0.8153\n",
      "Epoch 6/8\n",
      "1262892/1262892 [==============================] - 98s 77us/step - loss: 0.3832 - acc: 0.8279 - val_loss: 0.4073 - val_acc: 0.8150\n",
      "Epoch 7/8\n",
      "1262892/1262892 [==============================] - 97s 77us/step - loss: 0.3786 - acc: 0.8301 - val_loss: 0.4109 - val_acc: 0.8150\n",
      "Epoch 8/8\n",
      "1262892/1262892 [==============================] - 97s 77us/step - loss: 0.3748 - acc: 0.8324 - val_loss: 0.4103 - val_acc: 0.8145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x200264545f8>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb = TensorBoard(log_dir=path.join(MODELS_DIR,MODEL_TYPE_ID), histogram_freq=num_epochs,\n",
    "                 write_grads=True, write_images=True, batch_size = default_batch_size, embeddings_freq=default_batch_size, \n",
    "                 embeddings_metadata=None)\n",
    "hist = model.fit(x_train, y_train, validation_data=(x_test, y_test),\n",
    "          epochs=num_epochs, batch_size=default_batch_size,callbacks=[tb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./1d_ConvNet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(hist.history['acc'], lw=2.0, color='b', label='train')\n",
    "plt.plot(hist.history['val_acc'], lw=2.0, color='r', label='val')\n",
    "plt.title('CNN sentiment')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0, 176, 863]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trail = ['please die!']\n",
    "sequences = tokenizer.texts_to_sequences(trail)\n",
    "dummy = pad_sequences(sequences, maxlen=TWEET_LENGTH)\n",
    "dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step\n",
      "[0.8296     0.17040004] 0\n",
      "please die! prediction-> Negative\n"
     ]
    }
   ],
   "source": [
    "result = model.predict(dummy,verbose=1)[0]\n",
    "print(result,np.argmax(result))\n",
    "print(trail[0],'prediction->',labels_dict[np.argmax(result)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
